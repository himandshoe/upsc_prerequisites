{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7FPiAIwHteCl"
      },
      "source": [
        "# Mistral OCR Cookbook\n",
        "\n",
        "modified from the Mistral Cookbook [Notebook](https://colab.research.google.com/github/mistralai/cookbook/blob/main/mistral/ocr/structured_ocr.ipynb#scrollTo=po7Cukllt8za)\n",
        "\n",
        "---\n",
        "\n",
        "## OCR Exploration and Structured Outputs\n",
        "In this cookbook, we will explore the basics of OCR and leverage it together with existing models to achieve structured outputs fueled by our OCR model.\n",
        "\n",
        "You may want to do this in case current vision models are not powerful enough, hence enhancing their vision OCR capabilities with the OCR model to achieve better structured data extraction.\n",
        "\n",
        "---\n",
        "\n",
        "### Used\n",
        "- Mistral OCR\n",
        "- Pixtral 12B & Ministral 8B\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UgZW4ZfetwAl"
      },
      "source": [
        "### Setup\n",
        "First, let's install `mistralai` and download the required files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "po7Cukllt8za",
        "outputId": "b15665c3-f649-45d2-abba-db096698074f"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with '.venv (Python 3.12.10)' requires the ipykernel package.\n",
            "\u001b[1;31mInstall 'ipykernel' into the Python environment. \n",
            "\u001b[1;31mCommand: '/home/msi/upsc_prerequisites/.venv/bin/python -m pip install ipykernel -U --force-reinstall'"
          ]
        }
      ],
      "source": [
        "%pip -q install mistralai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i4DyZONIJ_kE"
      },
      "source": [
        "bring in key from Google Colab Secrets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bUnEOIy6tDBv"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "os.environ['MISTRAL_API_KEY'] = userdata.get('MISTRAL_API_KEY')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NhwM0aITt7ti"
      },
      "source": [
        "We can now set up our client. You can create an API key on our [Plateforme](https://console.mistral.ai/api-keys/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "odfkuCk6qSAw"
      },
      "outputs": [],
      "source": [
        "from mistralai import Mistral\n",
        "\n",
        "\n",
        "client = Mistral(api_key=os.environ['MISTRAL_API_KEY'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xk-3YwljuFKK"
      },
      "source": [
        "There are two types of files you can apply OCR to:\n",
        "- PDF files, either uploaded or from URLs..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_iJ9TdZBrfJ2"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "pdf_file = Path(\"upsc_textbook.pdf\")\n",
        "assert pdf_file.is_file()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "svaJGBFlqm7_",
        "outputId": "71c63bfc-e57d-4172-edc9-987a8c40a761"
      },
      "outputs": [],
      "source": [
        "from mistralai import DocumentURLChunk, ImageURLChunk, TextChunk\n",
        "import json\n",
        "\n",
        "uploaded_file = client.files.upload(\n",
        "    file={\n",
        "        \"file_name\": pdf_file.stem,\n",
        "        \"content\": pdf_file.read_bytes(),\n",
        "    },\n",
        "    purpose=\"ocr\",\n",
        ")\n",
        "\n",
        "signed_url = client.files.get_signed_url(file_id=uploaded_file.id, expiry=1)\n",
        "\n",
        "pdf_response = client.ocr.process(document=DocumentURLChunk(document_url=signed_url.url),\n",
        "                                  model=\"mistral-ocr-latest\",\n",
        "                                  include_image_base64=True)\n",
        "\n",
        "response_dict = json.loads(pdf_response.json())\n",
        "json_string = json.dumps(response_dict, indent=4)\n",
        "\n",
        "# print(json_string)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EG2_TdlKIxYs"
      },
      "source": [
        "*The OCR model can output interleaved text and images (set `include_image_base64=True` to return the base64 image ), we can view the result with the following:*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "dxefUpm-Idp8",
        "outputId": "f5c9e404-247a-4059-e9a4-b157dbeb9eeb"
      },
      "outputs": [],
      "source": [
        "from mistralai.models import OCRResponse\n",
        "from IPython.display import Markdown, display\n",
        "\n",
        "def replace_images_in_markdown(markdown_str: str, images_dict: dict) -> str:\n",
        "    for img_name, base64_str in images_dict.items():\n",
        "        markdown_str = markdown_str.replace(f\"![{img_name}]({img_name})\", f\"![{img_name}]({base64_str})\")\n",
        "    return markdown_str\n",
        "\n",
        "def get_combined_markdown(ocr_response: OCRResponse) -> str:\n",
        "  markdowns: list[str] = []\n",
        "  for page in pdf_response.pages:\n",
        "    image_data = {}\n",
        "    for img in page.images:\n",
        "      image_data[img.id] = img.image_base64\n",
        "    markdowns.append(replace_images_in_markdown(page.markdown, image_data))\n",
        "\n",
        "  return \"\\n\\n\".join(markdowns)\n",
        "\n",
        "display(Markdown(get_combined_markdown(pdf_response)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IQpR6rnJ9NtD",
        "outputId": "8cd955d0-0d4d-4fe8-becd-a99dc78c99c4"
      },
      "outputs": [],
      "source": [
        "import os # Import the os module for path operations (optional but good practice)\n",
        "from mistralai.models import OCRResponse\n",
        "from IPython.display import Markdown, display\n",
        "\n",
        "def replace_images_in_markdown(markdown_str: str, images_dict: dict) -> str:\n",
        "    \"\"\"Replaces image placeholders with base64 encoded strings.\"\"\"\n",
        "    for img_name, base64_str in images_dict.items():\n",
        "        # Ensure the base64 string is properly formatted for Markdown image links\n",
        "        # Usually starts with 'data:image/...'\n",
        "        markdown_str = markdown_str.replace(f\"![{img_name}]({img_name})\", f\"![{img_name}]({base64_str})\")\n",
        "    return markdown_str\n",
        "\n",
        "def get_combined_markdown(ocr_response: OCRResponse) -> str:\n",
        "  \"\"\"Combines markdown from all pages of an OCR response, embedding images.\"\"\"\n",
        "  markdowns: list[str] = []\n",
        "  # Assuming pdf_response is the correct variable name based on the original code context\n",
        "  # If ocr_response is the intended variable, use that instead. Let's assume pdf_response for now.\n",
        "  # Replace pdf_response with ocr_response if that's the actual variable holding the data\n",
        "  for page in ocr_response.pages: # Changed pdf_response to ocr_response based on function signature\n",
        "    image_data = {}\n",
        "    if page.images: # Check if there are images on the page\n",
        "        for img in page.images:\n",
        "          # Ensure both id and base64 string exist\n",
        "          if img.id and img.image_base64:\n",
        "            image_data[img.id] = img.image_base64\n",
        "    if page.markdown: # Check if markdown exists for the page\n",
        "        markdowns.append(replace_images_in_markdown(page.markdown, image_data))\n",
        "\n",
        "  return \"\\n\\n\".join(markdowns)\n",
        "\n",
        "def save_markdown_to_file(markdown_content: str, filename: str = \"output.md\"):\n",
        "  \"\"\"Saves the given markdown content to a file.\"\"\"\n",
        "  try:\n",
        "    with open(filename, 'w', encoding='utf-8') as f:\n",
        "      f.write(markdown_content)\n",
        "    print(f\"Successfully saved Markdown to '{filename}'\")\n",
        "  except IOError as e:\n",
        "    print(f\"Error saving Markdown to file '{filename}': {e}\")\n",
        "  except Exception as e:\n",
        "    print(f\"An unexpected error occurred: {e}\")\n",
        "\n",
        "# --- Main execution part ---\n",
        "\n",
        "# Assume 'pdf_response' is your OCRResponse object containing the data\n",
        "# Replace 'pdf_response' with the actual variable name if it's different\n",
        "# Example placeholder: pdf_response = OCRResponse(...) # Load or get your response here\n",
        "\n",
        "# 1. Generate the combined markdown string\n",
        "combined_markdown_output = get_combined_markdown(pdf_response) # Pass your OCRResponse object\n",
        "\n",
        "# 2. Save the combined markdown to a file\n",
        "output_filename = \"generated_report.md\" # Choose your desired filename\n",
        "save_markdown_to_file(combined_markdown_output, output_filename)\n",
        "\n",
        "# 3. Display the markdown in the IPython environment (optional)\n",
        "# display(Markdown(combined_markdown_output))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8yk5tBpPuKal"
      },
      "source": [
        "## For single Image files..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8vZ8gnTRrejO"
      },
      "outputs": [],
      "source": [
        "image_file = Path(\"rd-test-img.png\")\n",
        "assert image_file.is_file()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sFdyKIcgrahm",
        "outputId": "b46b2024-8a64-460b-84c2-681f1b5571e7"
      },
      "outputs": [],
      "source": [
        "import base64\n",
        "\n",
        "encoded = base64.b64encode(image_file.read_bytes()).decode()\n",
        "base64_data_url = f\"data:image/jpeg;base64,{encoded}\"\n",
        "\n",
        "image_response = client.ocr.process(document=ImageURLChunk(image_url=base64_data_url), model=\"mistral-ocr-latest\")\n",
        "\n",
        "response_dict = json.loads(image_response.json())\n",
        "json_string = json.dumps(response_dict, indent=4)\n",
        "print(json_string)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MdKKt-lPFv28"
      },
      "source": [
        "## Combining Pixtral for structure with the OCR"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pWStbt7LuMvT"
      },
      "source": [
        "We want to be able to extract structured data from these files. For this, we will make use of `pixtral-12b-latest` and support it with our OCR model for better, high-quality answers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8aZOQs38r0GO",
        "outputId": "8686cca4-c2cc-4c39-fa78-dcb776ef2ada"
      },
      "outputs": [],
      "source": [
        "image_ocr_markdown = image_response.pages[0].markdown\n",
        "\n",
        "chat_response = client.chat.complete(\n",
        "    model=\"pixtral-12b-latest\",\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "                ImageURLChunk(image_url=base64_data_url),\n",
        "                TextChunk(text=f\"This is image's OCR in markdown:\\n<BEGIN_IMAGE_OCR>\\n{image_ocr_markdown}\\n<END_IMAGE_OCR>.\\nConvert this into a sensible structured json response. The output should be strictly be json with no extra commentary\")\n",
        "            ],\n",
        "        },\n",
        "    ],\n",
        "    response_format =  {\"type\": \"json_object\"},\n",
        "    temperature=0\n",
        ")\n",
        "\n",
        "response_dict = json.loads(chat_response.choices[0].message.content)\n",
        "json_string = json.dumps(response_dict, indent=4)\n",
        "print(json_string)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X4eiQQo-F7cA"
      },
      "source": [
        "## Passing the OCR output into an LLM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4YKioib1vgTZ"
      },
      "source": [
        "Note: We are leveraging a model already capable of vision tasks. However, we could also use text-only models for the structured output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RH2-OlXqvqTT",
        "outputId": "f192dadf-ea71-4e5e-c72a-4546fe1da158"
      },
      "outputs": [],
      "source": [
        "image_ocr_markdown = image_response.pages[0].markdown\n",
        "\n",
        "chat_response = client.chat.complete(\n",
        "    model=\"ministral-8b-latest\",\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": f\"This is image's OCR in markdown:\\n<BEGIN_IMAGE_OCR>\\n{image_ocr_markdown}\\n<END_IMAGE_OCR>.\\nConvert this into a sensible structured json response. The output should be strictly be json with no extra commentary\"\n",
        "        },\n",
        "    ],\n",
        "    response_format =  {\"type\": \"json_object\"},\n",
        "    temperature=0\n",
        ")\n",
        "\n",
        "response_dict = json.loads(chat_response.choices[0].message.content)\n",
        "json_string = json.dumps(response_dict, indent=4)\n",
        "print(json_string)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pc__PKmkwUnZ"
      },
      "source": [
        "### All Together\n",
        "Let's design a simple function that takes an `image_path` file and returns a JSON structured output in a specific format. In this case, we arbitrarily decided we wanted an output respecting the following:\n",
        "\n",
        "```python\n",
        "class StructuredOCR:\n",
        "    file_name: str  # can be any string\n",
        "    topics: list[str]  # must be a list of strings\n",
        "    languages: list[Language]  # a list of languages\n",
        "    ocr_contents: dict  # any dictionary, can be freely defined by the model\n",
        "```\n",
        "\n",
        "We will make use of [custom structured outputs](https://docs.mistral.ai/capabilities/structured-output/custom_structured_output/) as well as `pycountry` for the languages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VsKWV5pyxp8u",
        "outputId": "4a41005b-8fea-44e9-d9ea-419c1336efc9"
      },
      "outputs": [],
      "source": [
        "!pip install pycountry"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oM2ensmIwh4H",
        "outputId": "1b9dd20c-5c83-4d79-e705-6ed62d169269"
      },
      "outputs": [],
      "source": [
        "from enum import Enum\n",
        "from pathlib import Path\n",
        "from pydantic import BaseModel\n",
        "import base64\n",
        "import pycountry\n",
        "\n",
        "languages = {lang.alpha_2: lang.name for lang in pycountry.languages if hasattr(lang, 'alpha_2')}\n",
        "\n",
        "class LanguageMeta(Enum.__class__):\n",
        "    def __new__(metacls, cls, bases, classdict):\n",
        "        for code, name in languages.items():\n",
        "            classdict[name.upper().replace(' ', '_')] = name\n",
        "        return super().__new__(metacls, cls, bases, classdict)\n",
        "\n",
        "class Language(Enum, metaclass=LanguageMeta):\n",
        "    pass\n",
        "\n",
        "class StructuredOCR(BaseModel):\n",
        "    file_name: str\n",
        "    topics: list[str]\n",
        "    languages: list[Language]\n",
        "    ocr_contents: dict\n",
        "\n",
        "print(StructuredOCR.schema_json())\n",
        "\n",
        "def structured_ocr(image_path: str) -> StructuredOCR:\n",
        "    image_file = Path(image_path)\n",
        "    assert image_file.is_file(), \"The provided image path does not exist.\"\n",
        "\n",
        "    # Read and encode the image file\n",
        "    encoded_image = base64.b64encode(image_file.read_bytes()).decode()\n",
        "    base64_data_url = f\"data:image/jpeg;base64,{encoded_image}\"\n",
        "\n",
        "    # Process the image using OCR\n",
        "    image_response = client.ocr.process(document=ImageURLChunk(image_url=base64_data_url), model=\"mistral-ocr-latest\")\n",
        "    image_ocr_markdown = image_response.pages[0].markdown\n",
        "\n",
        "    # Parse the OCR result into a structured JSON response\n",
        "    chat_response = client.chat.parse(\n",
        "        model=\"pixtral-12b-latest\",\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": [\n",
        "                    ImageURLChunk(image_url=base64_data_url),\n",
        "                    TextChunk(text=(\n",
        "                        \"This is the image's OCR in markdown:\\n\"\n",
        "                        f\"<BEGIN_IMAGE_OCR>\\n{image_ocr_markdown}\\n<END_IMAGE_OCR>.\\n\"\n",
        "                        \"Convert this into a structured JSON response with the OCR contents in a sensible dictionnary.\"\n",
        "                    ))\n",
        "                ],\n",
        "            },\n",
        "        ],\n",
        "        response_format=StructuredOCR,\n",
        "        temperature=0\n",
        "    )\n",
        "\n",
        "    return chat_response.choices[0].message.parsed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UVipACEOAyEX"
      },
      "source": [
        "We can now extract structured output from any image parsed with our OCR model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "1Xj9tOTKA7mw",
        "outputId": "a26a96fa-8e3b-42b7-8666-3fb003d23530"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "\n",
        "image_path = \"rd-test-img.png\"\n",
        "\n",
        "image = Image.open(image_path)\n",
        "image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uvt3OAcpyXCF",
        "outputId": "196040fe-560c-48f0-f5b4-e5861fa793db"
      },
      "outputs": [],
      "source": [
        "image_path = \"rd-test-img.png\"\n",
        "structured_response = structured_ocr(image_path)\n",
        "\n",
        "response_dict = json.loads(structured_response.json())\n",
        "json_string = json.dumps(response_dict, indent=4)\n",
        "print(json_string)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "zd9ZRHCbv_IO",
        "outputId": "bfdf7e87-982e-41fe-ff12-d44462a06a83"
      },
      "outputs": [],
      "source": [
        "image_path = \"/content/thai_learning.png\"\n",
        "image = Image.open(image_path)\n",
        "image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5HY4Aih0HLMv",
        "outputId": "4b314ba2-fe49-44e0-882f-82d6e0b3d339"
      },
      "outputs": [],
      "source": [
        "structured_response = structured_ocr(image_path)\n",
        "\n",
        "response_dict = json.loads(structured_response.json())\n",
        "json_string = json.dumps(response_dict, indent=4)\n",
        "print(json_string)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9n6dZRCjHQRg",
        "outputId": "4a17d2e2-5dc6-456a-bed7-b8a32d0671b5"
      },
      "outputs": [],
      "source": [
        "def format_thai_dictionary(thai_dict):\n",
        "    \"\"\"\n",
        "    Format and display Thai characters correctly from the provided JSON dictionary.\n",
        "\n",
        "    Args:\n",
        "        thai_dict (dict): Dictionary containing Thai language data\n",
        "\n",
        "    Returns:\n",
        "        str: Formatted string with properly aligned Thai characters\n",
        "    \"\"\"\n",
        "    output = []\n",
        "\n",
        "    # Add title\n",
        "    title = thai_dict.get(\"ocr_contents\", {}).get(\"title\", \"\")\n",
        "    output.append(f\"# {title}\")\n",
        "    output.append(\"\")\n",
        "\n",
        "    # Process sections\n",
        "    for section in thai_dict.get(\"ocr_contents\", {}).get(\"sections\", []):\n",
        "        section_title = section.get(\"title\", \"\")\n",
        "        output.append(f\"## {section_title}\")\n",
        "        output.append(\"\")\n",
        "\n",
        "        # Create header based on section type\n",
        "        if section_title == \"Thai Consonants\":\n",
        "            output.append(f\"{'Consonant':<15}{'Pronunciation':<20}{'Thai Character':<20}{'Meaning':<15}{'Translation':<15}\")\n",
        "            output.append(\"-\" * 85)\n",
        "\n",
        "            # Add consonant content\n",
        "            for item in section.get(\"content\", []):\n",
        "                consonant = item.get(\"consonant\", \"\")\n",
        "                pronunciation = item.get(\"pronunciation\", \"\")\n",
        "                thai_character = item.get(\"thai_character\", \"\")\n",
        "                meaning = item.get(\"meaning\", \"\")\n",
        "                translation = item.get(\"translation\", \"\")\n",
        "\n",
        "                output.append(f\"{consonant:<15}{pronunciation:<20}{thai_character:<20}{meaning:<15}{translation:<15}\")\n",
        "\n",
        "        elif section_title == \"Vowels\":\n",
        "            output.append(f\"{'Vowel':<15}{'Thai Character':<20}{'Meaning':<15}{'Translation':<15}\")\n",
        "            output.append(\"-\" * 65)\n",
        "\n",
        "            # Add vowel content\n",
        "            for item in section.get(\"content\", []):\n",
        "                vowel = item.get(\"vowel\", \"\")\n",
        "                thai_character = item.get(\"thai_character\", \"\")\n",
        "                meaning = item.get(\"meaning\", \"\")\n",
        "                translation = item.get(\"translation\", \"\")\n",
        "\n",
        "                output.append(f\"{vowel:<15}{thai_character:<20}{meaning:<15}{translation:<15}\")\n",
        "\n",
        "        output.append(\"\")  # Add empty line after each section\n",
        "\n",
        "    return \"\\n\".join(output)\n",
        "\n",
        "# Example usage\n",
        "\n",
        "#\n",
        "formatted_output = format_thai_dictionary(response_dict)\n",
        "print(formatted_output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t7xCR2VAHiim"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
